= Acceptance Testing for Continuous Delivery
Dave Farley • GOTO 2016 Youtube: https://www.youtube.com/watch?v=SBhgteA2szg
:doctype: presentation
:toc: left
:toclevels: 2

== Anti-Patterns in Acceptance Testing

* *Don’t* use **UI Record-and-Playback Systems**
* *Don’t* Record-and-Playback production data — this has a role, but it is **NOT** Acceptance Testing.
* *Don’t* dump production data into your test systems — instead, define the **absolute minimum data** required.
* *Don’t* assume **Nasty Automated Testing Products™** will do what you need.  
  Be sceptical and start with **your strategy**, then evaluate tools against it.
* *Don’t* have a **separate Testing/QA team** — quality is everyone’s responsibility.  
  Developers own Acceptance Tests!
* *Don’t* let every test start and initialize the app.  
  Optimise for **Cycle-Time** — be efficient in using test environments.
* *Don’t* include systems **outside your control** in your Acceptance Test scope.
* *Don't* Full end-to-end tests across multiple systems, they prevent precise control of state.
* *Don’t* put `wait()` instructions in tests hoping it will solve intermittency.

---

== Tricks for Success

* *Do* ensure that **developers own the tests**.  
* *Do* focus your tests on **“What” not “How”**.  
* *Do* think of your tests as **Executable Specifications**. They define, in machine-verifiable form, what “done” means from a user’s perspective, not a developer’s  
* *Do* make acceptance testing part of your **Definition of Done**.  
* *Do* keep tests **isolated from one another**.  
* *Do* keep your tests **repeatable**.  
* *Do* use the **language of the problem domain** — adopt the DSL approach regardless of technology.  
* *Do* **stub external systems**.  
* *Do* test in **production-like environments** (Use production like configurations etc...).  
* *Do* make instructions appear **synchronous at the test-case level** (do not wait for an async message from the queue, make it synchronous).  
* *Do* test for **any change**.  
* *Do* keep your tests **efficient**.
* *Do* Shift-left testing: collaborate during backlog grooming to write executable acceptance criteria before coding.
* *Do* Make acceptance tests **living documentation** — always up-to-date, always verified, and impossible to misinterpret. 
* *Do* you can acceptance-test background jobs and message-driven flows. Focus on business outcomes (“what”), not the trigger mechanism (“how”).
* *Do* cover user-visible bugs with tests—preferably by extending an existing acceptance test that already specifies that behaviour. Create a new acceptance test only when the bug reflects a new or missing business rule or a distinct user journey that isn’t captured yet.

== Test Environment Types

* Some tests need special treatment.  
* Tag tests with properties and allocate them dynamically.

=== Example Test Annotations

[source,java]
----
@TimeTravel
@Test
public void shouldDoSomethingThatNeedsFakeTime() {
    ...
}

@Destructive
@Test
public void shouldDoSomethingThatKillsPartOfTheSystem() {
    ...
}

@FPGA(version = 1.3)
@Test
public void shouldDoSomethingThatRequiresSpecificHardware() {
    ...
}
----

=== Key Concepts
* Identify tests that require **special environments** (e.g., time manipulation, destructive actions, hardware dependencies).
* Use **metadata or annotations** to dynamically allocate tests to proper environments.
* Manage specialized test resources automatically to ensure scalability and maintainability.
* Keep tests **self-describing** — environment needs should be explicit in metadata.

---

== Test Isolation

* Any form of testing is about **evaluating something in controlled circumstances**.  
* Isolation ensures predictability, reproducibility, and reliability.

=== Isolation Works on Multiple Levels
* **Isolating the System Under Test (SUT)** — test only what is within your responsibility.
* **Isolating test cases from each other** — enable parallel execution without resource conflicts.
* **Isolating test cases from themselves (temporal isolation)** — repeatable tests must not depend on prior state.

=== Strategic Insight
* **Isolation is a vital part of your test strategy.**
* Poor isolation leads to flaky tests, non-deterministic results, and unreliable feedback loops.
* Design environments, data, and infrastructure to support complete isolation of test executions.

== Thesis Summary and Strategic Insights

=== 1. Acceptance Tests Are *Executable Specifications*
Acceptance tests are not mere validations — they are *executable specifications* of system behaviour.  
They define, in machine-verifiable form, what “done” means from a **user’s perspective**, not a developer’s.

> “A good acceptance test is an executable specification for the behaviour of the system.”

==== Implications for Teams
* Treat tests as *contracts* between business and engineering.
* Automate them early and maintain them as core artefacts.
* Use domain-specific or business-readable language (DSLs, Gherkin, SpecFlow).

==== Extended Idea
In modern DevOps environments, executable specifications should also feed *live documentation* — API behaviour docs, compliance verification, and operational readiness dashboards.

---

=== 2. Continuous Delivery Is About *Fast Feedback Loops*
Farley frames development as a hierarchy of **feedback loops**:
* *Inner loop:* TDD → fast developer confidence (minutes)
* *Middle loop:* Acceptance testing → system-level confidence (hours)
* *Outer loop:* Continuous delivery → customer feedback (days/weeks)

The faster these loops operate, the faster and safer the organization can deliver.

==== Implications
* Optimize acceptance tests for feedback in **under one hour**.
* Continuously measure *time from commit to confidence*.
* Treat slow feedback as a *process defect*.

==== Extended Idea
Expose feedback loop metrics in CI/CD dashboards — include test duration, stability, and failure root-cause ratios.

---

=== 3. Developers Own Acceptance Tests
Farley strongly rejects the separation of QA automation and development.

> “Developers are the people who make changes that break tests; therefore, they must be the people responsible for making them pass.”

==== Implications
* Merge QA automation into engineering responsibility.
* Include acceptance test success in the *Definition of Done*.
* Involve QA early as *spec authors* and *test designers*, not downstream executors.

==== Extended Idea
Shift-left testing: collaborate during backlog grooming to write executable acceptance criteria *before* coding.

---

=== 4. Test Design: Focus on *What*, Not *How*
Anti-pattern: tests tightly coupled to implementation details (e.g., UI recorders, brittle APIs).

==== Thesis
Tests should express *intent* (“what”), not *mechanics* (“how”).

==== Implications
* Abstract communication channels (test “drivers” or adapters).
* Avoid UI-based automation; focus on domain-level behaviours.
* Fix interface changes in one place — not across all test cases.

==== Extended Idea
Treat test layers like clean architecture:
Acceptance tests depend on *business intent*, not *interface mechanics*.

---

=== 5. Isolation and Repeatability Are Non-Negotiable
> “Each test must be isolated from others, and rerunning it should yield identical results.”

==== Key Techniques
* **Functional aliasing:** dynamically generate unique entities (users, IDs, etc.) per test run. Example: User("John") - Value behind John-1d2Ad.
* **Controlled state:** avoid shared environments or test data.
* **Parallel execution:** enable concurrency safely.

==== Extended Idea
Use *ephemeral environments* — TestContainers, Kubernetes namespaces, or Terraform workspaces — for full test isolation.

---

=== 6. Acceptance Tests Drive System Design and Deployment Maturity
Acceptance tests act as *deployment rehearsals*.

> “By the time a release candidate reaches production, deployment should be a non-event.”

==== Implications
* Run acceptance tests in **production-like environments**.
* Automate deployments, configuration, and infrastructure validation.
* Treat acceptance tests as *deployment rehearsals* and *compliance gates*.

==== Extended Idea
Integrate acceptance tests with Infrastructure-as-Code pipelines.
Automate validation via *canary rollouts*, *smoke tests*, or *synthetic transactions*.

---

=== 7. Interface Testing and Decoupling Strategies
Full end-to-end tests across multiple systems create coupling and slow feedback.

> “Full end-to-end tests across multiple systems are anti-patterns when they prevent precise control of state.”

==== Strategy
* Each team tests its *own system boundaries*.
* Use *contract testing* to verify interfaces.
* Exchange interface contracts across teams via CI/CD pipelines.

==== Extended Idea
Adopt *consumer-driven contract testing* (e.g., Pact, Hoverfly, WireMock).
Teams validate dependencies autonomously while preserving integration confidence.

---

=== 8. The Role of Domain Language and DSLs
> “We use the language of the problem domain to express our needs in automated testing.”

==== Implications
* Build domain-specific languages (DSLs) to make tests readable and maintainable.
* Ensure both business and developers understand test intent.
* Keep test logic at the domain level, not technical API level.

==== Extended Idea
Combine DSLs with *model-based* and *AI-generated tests* to discover untested behaviour paths automatically.

---

=== 9. Efficiency as a Cultural Principle
A test suite that takes days to run indicates structural inefficiency.

> “Feedback under an hour is a game-changing level of feedback.”

==== Implications
* Optimize for execution time, parallelism, and targeted testing.
* Treat test performance as seriously as production performance.
* Continuously profile and tune test pipelines.

==== Extended Idea
Adopt *observability-driven testing*: measure test reliability, flakiness rate, and runtime as primary CI/CD metrics.

---

=== 10. Continuous Delivery Is the Sum of Disciplined Feedback Loops
Acceptance testing for CD is not just verification — it’s **designing for change**.

==== Core Synthesis
* *Executable specifications* → shared understanding  
* *Ownership* → closed feedback loops  
* *Isolation* → reliable automation  
* *Fast feedback* → faster innovation

==== Strategic Message for Management
Continuous Delivery succeeds when acceptance testing becomes an *engineering discipline*, not a QA phase.  
Leading organizations (e.g., LMAX) treat acceptance testing as part of *system design*, *deployment verification*, and *organizational learning*.

---

= Test Plan Matrix: What Goes Where
:toc:
:icons: font
:sectnums:

== Purpose
Map behaviors to the *lowest-cost layer* that proves them. Prevent E2E bloat; maximize fast coverage at unit/contract/integration layers.

== Non-Negotiable Principles

. **One scenario = one intent.**  
Describe the *what* once; implement it in exactly one test layer — always the *lowest layer that can fully prove the behavior*.

. **Tag-driven routing.**  
Use tags such as `@unit`, `@contract`, `@integration`, `@e2e`, `@smoke`, and `@critical`. The CI pipeline uses these tags to decide which runner executes each scenario.

. **No duplication across layers.**  
If a behavior is already guaranteed by *unit*, *contract*, or *integration* tests, do **not** repeat the same branches in E2E. The E2E layer contains only a thin set of *business-critical journeys*.

. **Examples-as-data.**  
Validations and rule matrices belong in **parameterized unit tests** using examples tables — *not* in E2E scenarios.

. **Living documentation.**  
The scenario specification is the *single source of truth*. Automation and CI reports link back to this specification so the documentation always reflects reality.

== Target Distribution (Guideline)
[cols="30,20,25,25",options="header"]
|===
| Layer | Share of Automated Tests | Typical Duration | Confidence Role
| Unit | 70–80% | milliseconds | Rules, validation, pure logic
| Contract | 5–10% | seconds | API shapes, error semantics at boundaries
| Integration | 10–15% | seconds–minutes | Service+DB workflows in isolation
| E2E Acceptance | 3–7% (5–20 scenarios) | minutes | User journeys & deploy/config sanity
|===

== Decision Matrix
[cols="30,35,15,20",options="header"]
|===
| Behavior Type | Example | Layer | Why
| Field validation | qty range, email format | Unit | Exhaustive, parameterized, fast
| Cross-field simple | start <= end | Unit | Deterministic matrix
| Domain rule (no I/O) | price calc, tax rule | Unit | Pure function
| DB-backed rule | stock reservation | Integration | Needs persistence seam
| External API contract | payment 4xx/5xx semantics | Contract | Boundary guarantees
| Workflow across services | create→validate→persist | Integration | End-to-end inside system boundary
| Full user journey | login→buy→invoice | E2E | Business outcome & config
| Compliance UX presence | age gate visible | E2E (spot) | Thin UI check only
|===

== Risk Model → Layer
[cols="22,22,22,34",options="header"]
|===
| Impact | Likelihood | Detectability in Prod | Layer
| High | High/Med | Hard | E2E or Integration (prefer Integration if UI not essential)
| High | Med/Low | Easy | Integration or Contract
| Med | Med/Low | Easy | Unit or Contract
| Low | Any | Any | Unit (or omit if redundant)
|===

== Example Mapping (Fill This Table)
[cols="14,12,18,28,28",options="header"]
|===
| Spec ID | Layer | Source Spec | Implementation | CI Report URL
| VAL-002 | unit | specs/validations-cart.adoc | QuantityValidatorTests.* | (link)
| TAX-014 | unit | specs/tax-rules.adoc | TaxCalculatorTests.* | (link)
| PAY-CT-101 | contract | specs/payment-contract.adoc | Payment.Pact.cs | (link)
| ORD-INT-020 | integration | specs/order-workflow.adoc | OrderWorkflowTests.* | (link)
| E2E-CHK-001 | e2e | specs/checkout-e2e.adoc | checkout.spec.ts | (link)
|===

== Promotion/Demotion Rules
* Move **down** the pyramid whenever a lower layer can prove the same behavior.
* Move **up** only when incidents show gaps or when deploy/config is integral to the risk.
* Every new E2E must include a removal or justification.

